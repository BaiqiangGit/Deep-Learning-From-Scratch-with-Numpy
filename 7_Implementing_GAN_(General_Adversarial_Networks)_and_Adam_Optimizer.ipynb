{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "Q7wHOBdzu85R",
    "outputId": "7a65edde-0874-4ca1-db00-f2bd6522eb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Input a Random Number to Seed123456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 19:30:44.996032 140239371409216 deprecation.py:323] From <ipython-input-1-c3f8be0efc50>:57: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0730 19:30:44.996637 140239371409216 deprecation.py:323] From /home/bq/anaconda3/envs/py37-numpy-deep-learning/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0730 19:30:44.997418 140239371409216 deprecation.py:323] From /home/bq/anaconda3/envs/py37-numpy-deep-learning/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0730 19:30:45.178691 140239371409216 deprecation.py:323] From /home/bq/anaconda3/envs/py37-numpy-deep-learning/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Load Data ----------\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0730 19:30:45.216616 140239371409216 deprecation.py:323] From /home/bq/anaconda3/envs/py37-numpy-deep-learning/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "--------- Declare Hyper Parameters ----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np,sys,time\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import numpy as np,sys\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "random_numer = int(input(\"Please Input a Random Number to Seed\"))\n",
    "\n",
    "\n",
    "np.random.seed(random_numer)\n",
    "def ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask *x\n",
    "def d_ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask \n",
    "\n",
    "def arctan(x):\n",
    "    return np.arctan(x)\n",
    "def d_arctan(x):\n",
    "    return 1 / (1 + x ** 2)\n",
    "\n",
    "def log(x):\n",
    "    return 1 / ( 1+ np.exp(-1*x))\n",
    "def d_log(x):\n",
    "    return log(x) * (1 - log(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def d_tanh(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load Data and declare hyper\n",
    "print('--------- Load Data ----------')\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
    "temp = mnist.test\n",
    "images, labels = temp.images, temp.labels\n",
    "images, labels = shuffle(np.asarray(images),np.asarray(labels))\n",
    "num_epoch = 10\n",
    "learing_rate = 0.00009\n",
    "G_input = 100\n",
    "hidden_input,hidden_input2,hidden_input3 = 128,256,346\n",
    "hidden_input4,hidden_input5,hidden_input6 = 480,560,686\n",
    "\n",
    "\n",
    "\n",
    "print('--------- Declare Hyper Parameters ----------')\n",
    "# 2. Declare Weights\n",
    "D_W1 = np.random.normal(size=(784,hidden_input),scale=(1. / np.sqrt(784 / 2.)))   *0.002\n",
    "# D_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))       *0.002\n",
    "D_b1 = np.zeros(hidden_input)\n",
    "\n",
    "D_W2 = np.random.normal(size=(hidden_input,1),scale=(1. / np.sqrt(hidden_input / 2.)))     *0.002\n",
    "# D_b2 = np.random.normal(size=(1),scale=(1. / np.sqrt(1 / 2.)))           *0.002\n",
    "D_b2 = np.zeros(1)\n",
    "\n",
    "\n",
    "G_W1 = np.random.normal(size=(G_input,hidden_input),scale=(1. / np.sqrt(G_input / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b1 = np.zeros(hidden_input)\n",
    "\n",
    "G_W2 = np.random.normal(size=(hidden_input,hidden_input2),scale=(1. / np.sqrt(hidden_input / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b2 = np.zeros(hidden_input2)\n",
    "\n",
    "G_W3 = np.random.normal(size=(hidden_input2,hidden_input3),scale=(1. / np.sqrt(hidden_input2 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b3 = np.zeros(hidden_input3)\n",
    "\n",
    "G_W4 = np.random.normal(size=(hidden_input3,hidden_input4),scale=(1. / np.sqrt(hidden_input3 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b4 = np.zeros(hidden_input4)\n",
    "\n",
    "G_W5 = np.random.normal(size=(hidden_input4,hidden_input5),scale=(1. / np.sqrt(hidden_input4 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b5 = np.zeros(hidden_input5)\n",
    "\n",
    "G_W6 = np.random.normal(size=(hidden_input5,hidden_input6),scale=(1. / np.sqrt(hidden_input5 / 2.)))   *0.002\n",
    "# G_b1 = np.random.normal(size=(128),scale=(1. / np.sqrt(128 / 2.)))      *0.002\n",
    "G_b6 = np.zeros(hidden_input6)\n",
    "\n",
    "G_W7 = np.random.normal(size=(hidden_input6,784),scale=(1. / np.sqrt(hidden_input6 / 2.)))  *0.002\n",
    "# G_b2 = np.random.normal(size=(784),scale=(1. / np.sqrt(784 / 2.)))      *0.002\n",
    "G_b7 = np.zeros(784)\n",
    "\n",
    "# 3. For Adam Optimzier\n",
    "v1,m1 = 0,0\n",
    "v2,m2 = 0,0\n",
    "v3,m3 = 0,0\n",
    "v4,m4 = 0,0\n",
    "\n",
    "v5,m5 = 0,0\n",
    "v6,m6 = 0,0\n",
    "v7,m7 = 0,0\n",
    "v8,m8 = 0,0\n",
    "v9,m9 = 0,0\n",
    "v10,m10 = 0,0\n",
    "v11,m11 = 0,0\n",
    "v12,m12 = 0,0\n",
    "\n",
    "v13,m13 = 0,0\n",
    "v14,m14 = 0,0\n",
    "\n",
    "v15,m15 = 0,0\n",
    "v16,m16 = 0,0\n",
    "\n",
    "v17,m17 = 0,0\n",
    "v18,m18 = 0,0\n",
    "\n",
    "\n",
    "beta_1,beta_2,eps = 0.9,0.999,0.00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "Q7wHOBdzu85R",
    "outputId": "7a65edde-0874-4ca1-db00-f2bd6522eb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Started Training ----------\n",
      "Current Iter:  0  Current D cost: [[4.79534229e-06]]  Current G cost:  [[0.69321891]]\r",
      "--------- Show Example Result See Tab Above ----------\n",
      "--------- Wait for the image to load ---------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('--------- Started Training ----------')\n",
    "for iter in range(num_epoch):\n",
    "\n",
    "    random_int = np.random.randint(len(images) - 5)\n",
    "    current_image = np.expand_dims(images[random_int],axis=0)\n",
    "\n",
    "    # Func: Generate The first Fake Data\n",
    "    Z = np.random.uniform(-1., 1., size=[1, G_input])\n",
    "    Gl1 = Z.dot(G_W1) + G_b1\n",
    "    Gl1A = arctan(Gl1)\n",
    "    Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "    Gl2A = ReLu(Gl2)\n",
    "    Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "    Gl3A = arctan(Gl3)\n",
    "\n",
    "    Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "    Gl4A = ReLu(Gl4)\n",
    "    Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "    Gl5A = tanh(Gl5)\n",
    "    Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "    Gl6A = ReLu(Gl6)\n",
    "    Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "\n",
    "    current_fake_data = log(Gl7)\n",
    "\n",
    "    # Func: Forward Feed for Real data\n",
    "    Dl1_r = current_image.dot(D_W1) + D_b1\n",
    "    Dl1_rA = ReLu(Dl1_r)\n",
    "    Dl2_r = Dl1_rA.dot(D_W2) + D_b2\n",
    "    Dl2_rA = log(Dl2_r)\n",
    "\n",
    "    # Func: Forward Feed for Fake Data\n",
    "    Dl1_f = current_fake_data.dot(D_W1) + D_b1\n",
    "    Dl1_fA = ReLu(Dl1_f)\n",
    "    Dl2_f = Dl1_fA.dot(D_W2) + D_b2\n",
    "    Dl2_fA = log(Dl2_f)\n",
    "\n",
    "    # Func: Cost D\n",
    "    D_cost = -np.log(Dl2_rA) + np.log(1.0- Dl2_fA)\n",
    "\n",
    "    # Func: Gradient\n",
    "    grad_f_w2_part_1 =  1/(1.0- Dl2_fA)\n",
    "    grad_f_w2_part_2 =  d_log(Dl2_f)\n",
    "    grad_f_w2_part_3 =   Dl1_fA\n",
    "    grad_f_w2 =       grad_f_w2_part_3.T.dot(grad_f_w2_part_1 * grad_f_w2_part_2) \n",
    "    grad_f_b2 = grad_f_w2_part_1 * grad_f_w2_part_2\n",
    "\n",
    "    grad_f_w1_part_1 =  (grad_f_w2_part_1 * grad_f_w2_part_2).dot(D_W2.T)\n",
    "    grad_f_w1_part_2 =  d_ReLu(Dl1_f)\n",
    "    grad_f_w1_part_3 =   current_fake_data\n",
    "    grad_f_w1 =       grad_f_w1_part_3.T.dot(grad_f_w1_part_1 * grad_f_w1_part_2) \n",
    "    grad_f_b1 =      grad_f_w1_part_1 * grad_f_w1_part_2\n",
    "\n",
    "    grad_r_w2_part_1 =  - 1/Dl2_rA\n",
    "    grad_r_w2_part_2 =  d_log(Dl2_r)\n",
    "    grad_r_w2_part_3 =   Dl1_rA\n",
    "    grad_r_w2 =       grad_r_w2_part_3.T.dot(grad_r_w2_part_1 * grad_r_w2_part_2) \n",
    "    grad_r_b2 =       grad_r_w2_part_1 * grad_r_w2_part_2\n",
    "\n",
    "    grad_r_w1_part_1 =  (grad_r_w2_part_1 * grad_r_w2_part_2).dot(D_W2.T)\n",
    "    grad_r_w1_part_2 =  d_ReLu(Dl1_r)\n",
    "    grad_r_w1_part_3 =   current_image\n",
    "    grad_r_w1 =       grad_r_w1_part_3.T.dot(grad_r_w1_part_1 * grad_r_w1_part_2) \n",
    "    grad_r_b1 =       grad_r_w1_part_1 * grad_r_w1_part_2\n",
    "\n",
    "    grad_w1 =grad_f_w1 + grad_r_w1\n",
    "    grad_b1 =grad_f_b1 + grad_r_b1\n",
    "    \n",
    "    grad_w2 =grad_f_w2 + grad_r_w2\n",
    "    grad_b2 =grad_f_b2 + grad_r_b2\n",
    "\n",
    "    # ---- Update Gradient ----\n",
    "    m1 = beta_1 * m1 + (1 - beta_1) * grad_w1\n",
    "    v1 = beta_2 * v1 + (1 - beta_2) * grad_w1 ** 2\n",
    "\n",
    "    m2 = beta_1 * m2 + (1 - beta_1) * grad_b1\n",
    "    v2 = beta_2 * v2 + (1 - beta_2) * grad_b1 ** 2\n",
    "\n",
    "    m3 = beta_1 * m3 + (1 - beta_1) * grad_w2\n",
    "    v3 = beta_2 * v3 + (1 - beta_2) * grad_w2 ** 2\n",
    "\n",
    "    m4 = beta_1 * m4 + (1 - beta_1) * grad_b2\n",
    "    v4 = beta_2 * v4 + (1 - beta_2) * grad_b2 ** 2\n",
    "\n",
    "    D_W1 = D_W1 - (learing_rate / (np.sqrt(v1 /(1-beta_2) ) + eps)) * (m1/(1-beta_1))\n",
    "    D_b1 = D_b1 - (learing_rate / (np.sqrt(v2 /(1-beta_2) ) + eps)) * (m2/(1-beta_1))\n",
    "    \n",
    "    D_W2 = D_W2 - (learing_rate / (np.sqrt(v3 /(1-beta_2) ) + eps)) * (m3/(1-beta_1))\n",
    "    D_b2 = D_b2 - (learing_rate / (np.sqrt(v4 /(1-beta_2) ) + eps)) * (m4/(1-beta_1))\n",
    "\n",
    "    # Func: Forward Feed for G\n",
    "    Z = np.random.uniform(-1., 1., size=[1, G_input])\n",
    "    Gl1 = Z.dot(G_W1) + G_b1\n",
    "    Gl1A = arctan(Gl1)\n",
    "    Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "    Gl2A = ReLu(Gl2)\n",
    "    Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "    Gl3A = arctan(Gl3)\n",
    "\n",
    "    Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "    Gl4A = ReLu(Gl4)\n",
    "    Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "    Gl5A = tanh(Gl5)\n",
    "    Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "    Gl6A = ReLu(Gl6)\n",
    "    Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "    \n",
    "    current_fake_data = log(Gl7)\n",
    "\n",
    "    Dl1 = current_fake_data.dot(D_W1) + D_b1\n",
    "    Dl1_A = ReLu(Dl1)\n",
    "    Dl2 = Dl1_A.dot(D_W2) + D_b2\n",
    "    Dl2_A = log(Dl2)\n",
    "\n",
    "    # Func: Cost G\n",
    "    G_cost = -np.log(Dl2_A)\n",
    "\n",
    "    # Func: Gradient\n",
    "    grad_G_w7_part_1 = ((-1/Dl2_A) * d_log(Dl2).dot(D_W2.T) * (d_ReLu(Dl1))).dot(D_W1.T)\n",
    "    grad_G_w7_part_2 = d_log(Gl7)\n",
    "    grad_G_w7_part_3 = Gl6A\n",
    "    grad_G_w7 = grad_G_w7_part_3.T.dot(grad_G_w7_part_1 * grad_G_w7_part_1)\n",
    "    grad_G_b7 = grad_G_w7_part_1 * grad_G_w7_part_2\n",
    "\n",
    "    grad_G_w6_part_1 = (grad_G_w7_part_1 * grad_G_w7_part_2).dot(G_W7.T)\n",
    "    grad_G_w6_part_2 = d_ReLu(Gl6)\n",
    "    grad_G_w6_part_3 = Gl5A\n",
    "    grad_G_w6 = grad_G_w6_part_3.T.dot(grad_G_w6_part_1 * grad_G_w6_part_2)\n",
    "    grad_G_b6 = (grad_G_w6_part_1 * grad_G_w6_part_2)\n",
    "\n",
    "    grad_G_w5_part_1 = (grad_G_w6_part_1 * grad_G_w6_part_2).dot(G_W6.T)\n",
    "    grad_G_w5_part_2 = d_tanh(Gl5)\n",
    "    grad_G_w5_part_3 = Gl4A\n",
    "    grad_G_w5 = grad_G_w5_part_3.T.dot(grad_G_w5_part_1 * grad_G_w5_part_2)\n",
    "    grad_G_b5 = (grad_G_w5_part_1 * grad_G_w5_part_2)\n",
    "\n",
    "    grad_G_w4_part_1 = (grad_G_w5_part_1 * grad_G_w5_part_2).dot(G_W5.T)\n",
    "    grad_G_w4_part_2 = d_ReLu(Gl4)\n",
    "    grad_G_w4_part_3 = Gl3A\n",
    "    grad_G_w4 = grad_G_w4_part_3.T.dot(grad_G_w4_part_1 * grad_G_w4_part_2)\n",
    "    grad_G_b4 = (grad_G_w4_part_1 * grad_G_w4_part_2)\n",
    "\n",
    "    grad_G_w3_part_1 = (grad_G_w4_part_1 * grad_G_w4_part_2).dot(G_W4.T)\n",
    "    grad_G_w3_part_2 = d_arctan(Gl3)\n",
    "    grad_G_w3_part_3 = Gl2A\n",
    "    grad_G_w3 = grad_G_w3_part_3.T.dot(grad_G_w3_part_1 * grad_G_w3_part_2)\n",
    "    grad_G_b3 = (grad_G_w3_part_1 * grad_G_w3_part_2)\n",
    "\n",
    "    grad_G_w2_part_1 = (grad_G_w3_part_1 * grad_G_w3_part_2).dot(G_W3.T)\n",
    "    grad_G_w2_part_2 = d_ReLu(Gl2)\n",
    "    grad_G_w2_part_3 = Gl1A\n",
    "    grad_G_w2 = grad_G_w2_part_3.T.dot(grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "    grad_G_b2 = (grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "\n",
    "    grad_G_w1_part_1 = (grad_G_w2_part_1 * grad_G_w2_part_2).dot(G_W2.T)\n",
    "    grad_G_w1_part_2 = d_arctan(Gl1)\n",
    "    grad_G_w1_part_3 = Z\n",
    "    grad_G_w1 = grad_G_w1_part_3.T.dot(grad_G_w1_part_1 * grad_G_w1_part_2)\n",
    "    grad_G_b1 = grad_G_w1_part_1 * grad_G_w1_part_2\n",
    "\n",
    "    # ---- Update Gradient ----\n",
    "    m5 = beta_1 * m5 + (1 - beta_1) * grad_G_w1\n",
    "    v5 = beta_2 * v5 + (1 - beta_2) * grad_G_w1 ** 2\n",
    "\n",
    "    m6 = beta_1 * m6 + (1 - beta_1) * grad_G_b1\n",
    "    v6 = beta_2 * v6 + (1 - beta_2) * grad_G_b1 ** 2\n",
    "\n",
    "    m7 = beta_1 * m7 + (1 - beta_1) * grad_G_w2\n",
    "    v7 = beta_2 * v7 + (1 - beta_2) * grad_G_w2 ** 2\n",
    "\n",
    "    m8 = beta_1 * m8 + (1 - beta_1) * grad_G_b2\n",
    "    v8 = beta_2 * v8 + (1 - beta_2) * grad_G_b2 ** 2\n",
    "\n",
    "    m9 = beta_1 * m9 + (1 - beta_1) * grad_G_w3\n",
    "    v9 = beta_2 * v9 + (1 - beta_2) * grad_G_w3 ** 2\n",
    "\n",
    "    m10 = beta_1 * m10 + (1 - beta_1) * grad_G_b3\n",
    "    v10 = beta_2 * v10 + (1 - beta_2) * grad_G_b3 ** 2\n",
    "\n",
    "    m11 = beta_1 * m11 + (1 - beta_1) * grad_G_w4\n",
    "    v11 = beta_2 * v11 + (1 - beta_2) * grad_G_w4 ** 2\n",
    "\n",
    "    m12 = beta_1 * m12 + (1 - beta_1) * grad_G_b4\n",
    "    v12 = beta_2 * v12 + (1 - beta_2) * grad_G_b4 ** 2\n",
    "\n",
    "    m13 = beta_1 * m13 + (1 - beta_1) * grad_G_w5\n",
    "    v13 = beta_2 * v13 + (1 - beta_2) * grad_G_w5 ** 2\n",
    "\n",
    "    m14 = beta_1 * m14 + (1 - beta_1) * grad_G_b5\n",
    "    v14 = beta_2 * v14 + (1 - beta_2) * grad_G_b5 ** 2\n",
    "\n",
    "    m15 = beta_1 * m15 + (1 - beta_1) * grad_G_w6\n",
    "    v15 = beta_2 * v15 + (1 - beta_2) * grad_G_w6 ** 2\n",
    "\n",
    "    m16 = beta_1 * m16 + (1 - beta_1) * grad_G_b6\n",
    "    v16 = beta_2 * v16 + (1 - beta_2) * grad_G_b6 ** 2\n",
    "\n",
    "    m17 = beta_1 * m17 + (1 - beta_1) * grad_G_w7\n",
    "    v17 = beta_2 * v17 + (1 - beta_2) * grad_G_w7 ** 2\n",
    "\n",
    "    m18 = beta_1 * m18 + (1 - beta_1) * grad_G_b7\n",
    "    v18 = beta_2 * v18 + (1 - beta_2) * grad_G_b7 ** 2\n",
    "\n",
    "    G_W1 = G_W1 - (learing_rate / (np.sqrt(v5 /(1-beta_2) ) + eps)) * (m5/(1-beta_1))\n",
    "    G_b1 = G_b1 - (learing_rate / (np.sqrt(v6 /(1-beta_2) ) + eps)) * (m6/(1-beta_1))\n",
    "    \n",
    "    G_W2 = G_W2 - (learing_rate / (np.sqrt(v7 /(1-beta_2) ) + eps)) * (m7/(1-beta_1))\n",
    "    G_b2 = G_b2 - (learing_rate / (np.sqrt(v8 /(1-beta_2) ) + eps)) * (m8/(1-beta_1))\n",
    "\n",
    "    G_W3 = G_W3 - (learing_rate / (np.sqrt(v9 /(1-beta_2) ) + eps)) * (m9/(1-beta_1))\n",
    "    G_b3 = G_b3 - (learing_rate / (np.sqrt(v10 /(1-beta_2) ) + eps)) * (m10/(1-beta_1))\n",
    "\n",
    "    G_W4 = G_W4 - (learing_rate / (np.sqrt(v11 /(1-beta_2) ) + eps)) * (m11/(1-beta_1))\n",
    "    G_b4 = G_b4 - (learing_rate / (np.sqrt(v12 /(1-beta_2) ) + eps)) * (m12/(1-beta_1))\n",
    "\n",
    "    G_W5 = G_W5 - (learing_rate / (np.sqrt(v13 /(1-beta_2) ) + eps)) * (m13/(1-beta_1))\n",
    "    G_b5 = G_b5 - (learing_rate / (np.sqrt(v14 /(1-beta_2) ) + eps)) * (m14/(1-beta_1))\n",
    "\n",
    "    G_W6 = G_W6 - (learing_rate / (np.sqrt(v15 /(1-beta_2) ) + eps)) * (m15/(1-beta_1))\n",
    "    G_b6 = G_b6 - (learing_rate / (np.sqrt(v16 /(1-beta_2) ) + eps)) * (m16/(1-beta_1))\n",
    "\n",
    "    G_W7 = G_W7 - (learing_rate / (np.sqrt(v17 /(1-beta_2) ) + eps)) * (m17/(1-beta_1))\n",
    "    G_b7 = G_b7 - (learing_rate / (np.sqrt(v18 /(1-beta_2) ) + eps)) * (m18/(1-beta_1))\n",
    "\n",
    "    # --- Print Error ----\n",
    "    #print(\"Current Iter: \",iter, \" Current D cost:\",D_cost, \" Current G cost: \", G_cost,end='\\r')\n",
    "    \n",
    "    if iter == 0:\n",
    "        learing_rate = learing_rate * 0.01\n",
    "    if iter == 40:\n",
    "        learing_rate = learing_rate * 0.01\n",
    "\n",
    "    # ---- Print to Out put ----\n",
    "    if iter%10 == 0:\n",
    "        \n",
    "        print(\"Current Iter: \",iter, \" Current D cost:\",D_cost, \" Current G cost: \", G_cost,end='\\r')\n",
    "        print('--------- Show Example Result See Tab Above ----------')\n",
    "        print('--------- Wait for the image to load ---------')\n",
    "        Z = np.random.uniform(-1., 1., size=[16, G_input]) \n",
    "\n",
    "        Gl1 = Z.dot(G_W1) + G_b1\n",
    "        Gl1A = arctan(Gl1)\n",
    "        Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "        Gl2A = ReLu(Gl2)\n",
    "        Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "        Gl3A = arctan(Gl3)\n",
    "\n",
    "        Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "        Gl4A = ReLu(Gl4)\n",
    "        Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "        Gl5A = tanh(Gl5)\n",
    "        Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "        Gl6A = ReLu(Gl6)\n",
    "        Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "        \n",
    "        current_fake_data = log(Gl7)\n",
    "        \n",
    "        fig = plot(current_fake_data)\n",
    "        fig.savefig('Click_Me_{}.png'.format(str(iter).zfill(3)+\"_Ginput_\"+str(G_input)+ \\\n",
    "        \"_hiddenone\"+str(hidden_input) + \"_hiddentwo\"+str(hidden_input2) + \"_LR_\" + str(learing_rate)\n",
    "        ), bbox_inches='tight')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# -- end code --"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "7  Implementing GAN (General Adversarial Networks) and Adam Optimizer ",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
