{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief intro\n",
    "\n",
    "Neural networks mimic the way the human brain works. They inherit some important characteristics of human brain, e.g. multiple neurons, multiple layers, information propagation and aggregation, and target-aware network adaptation. Fully Connected Neural Networks, AKA the multi-layer perceptrons (MLP), represents the vanilla architecture of neural networks, where a neuron always connects to all the neurons in the next layer of the neural network. In comparison to the CNN networks, the neurons have full access to the information in its previous layer. While due to co-adaptation of the neurons during network training, the neurons will evolve and differentiate with each other to form an asymptotic function of the target 'Optimal' solution for the given knowledge pool (the given training set. While whether the training set reprents well the real-world problem or not is another question which is beyound the scope of the learning process to reveal.). \n",
    "\n",
    "#### Layer Initialization\n",
    "The neurons within a layer are functionally the same. If we initialize the neurons with same weights, then all neurons will evolve the same way, and the whole layer will function exactly like one single neuron. This prevents the neurons from evolving differently to form a complex function. In the initialization stage, one of the target is to handle this 'symmetry problem' by initialize the weights of neurons in each layer with different weight values, e.g randnorm, truncatednorm.\n",
    "\n",
    "#### Personal Thoughts\n",
    "Theoretically, neurons within each layer have equal potential to become a specific neuron needed in this layer before initialization. However, due to the initialization and the training process, they will evolve differently as determined by the global loss criteria. Like cells in human body, the neurons also gradually differentiate with each other with time, and acquire specific functionalities, e.g specific and stable weight configurations. The interesting thing is, they will follow the system needs and have no resistance to loss their flexibilities and become specified, just like tools used by a soul (target function). \n",
    "\n",
    "#### Let code say\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load libs supporting the implementation\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') ## ignore warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata ## to load mnist\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load mnist mdataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = fetch_mldata('mnist original', data_home = 'datasets')\n",
    "X, y = mnist['data'], mnist['target'] ##  X: features, y: targets\n",
    "X.shape, y.shape ## tensor shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 70000), (10, 70000))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.T  ## reshape X as (n_features, n_samples)\n",
    "X = X/255.0  ## scale features into [0,1] \n",
    "Y = OneHotEncoder().fit_transform(y.reshape(-1,1).astype('int32')).toarray().T ## onehot encode y into Y of shape: (n_classes, n_samples)\n",
    "X.shape, Y.shape ## tensor shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (10, 60000), (784, 10000), (10, 10000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 60000 ## cut point for train/test sets\n",
    "X_train, Y_train = X[:,:m], Y[:,:m]\n",
    "X_test , Y_test  = X[:,m:], Y[:,m:]\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape ## tensor shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly shuffle trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (10, 60000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12345) ## for reproducibility\n",
    "shuffle = np.random.permutation(m) ## create shuffle index\n",
    "X_train, Y_train = X_train[:, shuffle], Y_train[:, shuffle]\n",
    "X_train.shape, Y_train.shape ## shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 1234\n",
    "plt.imshow(X_train[:,sample].reshape(28,-1), cmap = 'binary_r')\n",
    "plt.axis('off')\n",
    "plt.title(np.argmax(Y_train[:,sample]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement MLP\n",
    "\n",
    "So far so good. After preparation of the training/testing sets, it's time to code the neural network, which comprises of:\n",
    "\n",
    "    Input layer, with forward connections to Hidden layer\n",
    "    Hidden layer, with backward connections to Input layer, and forward connections to Output layer\n",
    "    Output layer, with backward connections to Hidden layer.\n",
    "    \n",
    "The loss function is choosen as the cross-entropy loss, and optimizer is set with vanilla gradient descent with a constant learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples   = 60000\n",
    "input_dims  = 784 \n",
    "hidden_dims = 128\n",
    "output_dims = 10\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weights/bias (they are what to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(hidden_dims, input_dims) ## info are squeezed in each neuron, as hidden_dims << input_dims\n",
    "b1 = np.zeros((hidden_dims,1)) ## bias 1\n",
    "W2 = np.random.randn(output_dims, hidden_dims) ## to target \n",
    "b2 = np.zeros((output_dims, 1)) ## bias 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , Loss 0.5250180451773613\n",
      "Epoch 20 , Loss 0.5041418260634201\n",
      "Epoch 40 , Loss 0.4859441196050856\n",
      "Epoch 60 , Loss 0.4698878378871053\n",
      "Epoch 80 , Loss 0.45557302362342583\n",
      "Epoch 100 , Loss 0.4426963609993937\n",
      "Epoch 120 , Loss 0.4310239159318427\n",
      "Epoch 140 , Loss 0.4203722575343032\n",
      "Epoch 160 , Loss 0.4105952032886478\n",
      "Epoch 180 , Loss 0.4015744734136054\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "\n",
    "    ## Calculate output\n",
    "    Z1 = W1 @ X_train + b1 ## affine (rotate, scale, translate)\n",
    "    A1 = 1 / (1 + np.exp(-Z1)) ## sigmoid\n",
    "    Z2 = W2 @ A1 + b2 ## affine\n",
    "    A2 = np.exp(Z2)/np.exp(Z2).sum(axis = 0) ## softmax\n",
    "\n",
    "    ## Calculate Loss \n",
    "    Loss = - np.sum(Y_train * np.log(A2))/n_samples ## cross-entropy loss\n",
    "\n",
    "    ## Calculate derivatives\n",
    "    dZ2 = A2 - Y_train ## loss derivative to Z2\n",
    "    dW2 = dZ2 @ A1.T/n_samples ## loss derivative to W2 \n",
    "    db2 = dZ2.mean(axis = 1, keepdims = True) ## keep the shape\n",
    "\n",
    "    dA1 = W2.T @ dZ2 ## chain rule\n",
    "    dZ1 = dA1 * A1 * (1 - A1) ## chain rule\n",
    "    dW1 = dZ1 @ X_train.T / n_samples\n",
    "    db1 = dZ1.mean(axis = 1, keepdims = True)\n",
    "\n",
    "    ## Update weights/bias\n",
    "    W1 -= dW1 * lr\n",
    "    b1 -= db1 * lr\n",
    "    W2 -= dW2 * lr\n",
    "    b2 -= db2 * lr\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print('Epoch', epoch, ', Loss', Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = W1 @ X_test + b1\n",
    "A1 = 1 / (1 + np.exp(-Z1))\n",
    "Z2 = W2 @ A1 + b2\n",
    "A2 = np.exp(Z2)/np.exp(Z2).sum(axis = 0)\n",
    "\n",
    "preds = np.argmax(A2, axis = 0)\n",
    "truth = np.argmax(Y_test, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8838\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(truth, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 930    1    8    4    1   13   13    6    3    1]\n",
      " [   0 1100    9    5    1    4    2    1   13    0]\n",
      " [  11    5  883   33   17    7   17   18   32    9]\n",
      " [   1    1   28  880    1   50    2   17   23    7]\n",
      " [   1    3   16    6  853    6   13    9   16   59]\n",
      " [  10   10    9   61   14  715   16    9   42    6]\n",
      " [  13    5    9    1   19   21  883    1    6    0]\n",
      " [   1    7   27    7   16    1    1  909    7   52]\n",
      " [   7    7   13   36   10   40   13    9  821   18]\n",
      " [  15    5    4   12   45   10    0   40   14  864]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(truth, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       980\n",
      "           1       0.96      0.97      0.97      1135\n",
      "           2       0.88      0.86      0.87      1032\n",
      "           3       0.84      0.87      0.86      1010\n",
      "           4       0.87      0.87      0.87       982\n",
      "           5       0.82      0.80      0.81       892\n",
      "           6       0.92      0.92      0.92       958\n",
      "           7       0.89      0.88      0.89      1028\n",
      "           8       0.84      0.84      0.84       974\n",
      "           9       0.85      0.86      0.85      1009\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(truth, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "The results are not too bad, while there are many improvements can be done:\n",
    "\n",
    "To improve accuracy:\n",
    "\n",
    "    1) change number of hidden neurons\n",
    "    2) change number of hidden layers\n",
    "    3) change activation functions\n",
    "    4) change loss function, e.g hinge loss\n",
    "    5) change optimizer, e.g adam\n",
    "    6) change learning rate\n",
    "    7) using CNN or RNN\n",
    "    8) dropout and batchnorm\n",
    "\n",
    "To improve speed:\n",
    "    \n",
    "    1) SGD (batch-wise computation)\n",
    "    2) use GPU implementation e.g cupy\n",
    "    \n",
    "To improve readability:\n",
    "    \n",
    "    1) encapsulate the buiding blocks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
