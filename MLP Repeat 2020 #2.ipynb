{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T13:08:44.657573Z",
     "start_time": "2020-01-31T13:08:44.654103Z"
    }
   },
   "outputs": [],
   "source": [
    "## repeat for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T13:08:45.112293Z",
     "start_time": "2020-01-31T13:08:45.102695Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T13:09:03.649133Z",
     "start_time": "2020-01-31T13:08:45.536817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load mnist\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist.data, mnist.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T13:09:03.933663Z",
     "start_time": "2020-01-31T13:09:03.651727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 70000), (10, 70000))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "X = X.T / 255.0\n",
    "y = OneHotEncoder().fit_transform(y.astype('int32').reshape(-1,1)).toarray().T\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T13:09:05.256039Z",
     "start_time": "2020-01-31T13:09:03.935734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (10, 60000))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make train/test split\n",
    "m = 60000\n",
    "X_train, X_test = X[:,:m], X[:,m:]\n",
    "y_train, y_test = y[:,:m], y[:,m:]\n",
    "\n",
    "# shuffle\n",
    "seed = 123456\n",
    "np.random.seed(seed)\n",
    "shuffle = np.random.permutation(m)\n",
    "X_train, y_train = X_train[:, shuffle], y_train[:,shuffle]\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T13:09:05.265912Z",
     "start_time": "2020-01-31T13:09:05.258373Z"
    }
   },
   "outputs": [],
   "source": [
    "# build nlp\n",
    "n_samples = 60000\n",
    "input_dims = 784\n",
    "hidden_dims = 64\n",
    "output_dims = 10\n",
    "\n",
    "# weights/bias\n",
    "W1 = np.random.randn(hidden_dims, input_dims)\n",
    "b1 = np.zeros((hidden_dims, 1))\n",
    "W2 = np.random.randn(output_dims, hidden_dims)\n",
    "b2 = np.zeros((output_dims, 1))\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T13:44:50.148396Z",
     "start_time": "2020-01-31T13:40:34.078462Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The loss at epoch # 0 is 0.0526                                                                                                    \n",
      "The loss at epoch #100 is 0.0525                                                                                                    \n",
      "The loss at epoch #200 is 0.0525                                                                                                    \n",
      "The loss at epoch #300 is 0.0525                                                                                                    \n",
      "The loss at epoch #400 is 0.0525                                                                                                    \n",
      "The loss at epoch #500 is 0.0525                                                                                                    \n",
      "The loss at epoch #600 is 0.0524                                                                                                    \n",
      "The loss at epoch #700 is 0.0524                                                                                                    \n",
      "The loss at epoch #800 is 0.0524                                                                                                    \n",
      "The loss at epoch #900 is 0.0524                                                                                                    \n",
      "\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       980\n",
      "           1       0.99      0.98      0.98      1135\n",
      "           2       0.95      0.95      0.95      1032\n",
      "           3       0.94      0.95      0.95      1010\n",
      "           4       0.96      0.96      0.96       982\n",
      "           5       0.95      0.95      0.95       892\n",
      "           6       0.97      0.96      0.97       958\n",
      "           7       0.96      0.95      0.96      1028\n",
      "           8       0.94      0.95      0.94       974\n",
      "           9       0.95      0.94      0.94      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n",
      "\n",
      "\n",
      "accuracy_score\n",
      "0.9578\n",
      "\n",
      "\n",
      "confusion_matrix\n",
      "[[ 961    1    2    2    2    2    2    2    2    4]\n",
      " [   0 1115    3    4    0    2    2    1    8    0]\n",
      " [   3    3  981   12    5    4    3   10   11    0]\n",
      " [   1    1   11  958    0   13    0    4   15    7]\n",
      " [   1    0    5    0  945    1    7    3    3   17]\n",
      " [   5    0    0   18    1  844    6    1   12    5]\n",
      " [  10    2    1    0    5   11  922    1    6    0]\n",
      " [   2    5   20    5    2    0    0  979    0   15]\n",
      " [   5    1    6    9    5    6    6    6  923    7]\n",
      " [   4    3    0    8   22    7    0   13    2  950]]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lr = 0.1\n",
    "for ep in range(1000):\n",
    "\n",
    "    # forward pass\n",
    "    Z1 = W1 @ X_train + b1 # (128, 784) @ (784, 60000) + (128, 1)\n",
    "    A1 = 1 / (1 + np.exp(-Z1)) # sigmoid: 128 * 60000\n",
    "    Z2 = W2 @ A1 + b2 # (10, 32) @ (128, 60000) + (10, 1)\n",
    "    A2 = np.exp(Z2) / np.exp(Z2).sum(axis = 0) # 10 * 60000, prob for each class\n",
    "\n",
    "    # calculate loss\n",
    "    L  = -np.sum(y_train * np.log(A2))/n_samples # scaler\n",
    "    \n",
    "    # backward pass\n",
    "    dZ2 = A2 - y_train # 10 * 60000, dL/dZ2 = Y_hat - Y (square-error-like)\n",
    "    dW2 = dZ2 @ A1.T / n_samples # (10,60000) @ (10, 128).T / 60000\n",
    "    db2 = dZ2.sum(axis = 1, keepdims = True)/n_samples # (10, 1) <== (10, 60000).sum(axis = 1, keepdims= True)\n",
    "\n",
    "    dA1 = W2.T @ dZ2 # (10 * 784).T @ (10, 60000) ==> (784, 60000)\n",
    "    dZ1 = dA1 * A1 * (1 - A1) # d_sigmoid\n",
    "    dW1 = dZ1 @ X_train.T / n_samples\n",
    "    db1 = dZ1.sum(axis=1, keepdims = True)/n_samples\n",
    "\n",
    "    # update W/b\n",
    "    W1 -= lr * dW1\n",
    "    W2 -= lr * dW2\n",
    "    b1 -= lr * db1\n",
    "    b2 -= lr * db2\n",
    "    \n",
    "    # print\n",
    "    print('\\nThe loss at epoch #%2d is %2.4f'%(ep, L) if ep%100 == 0 else '', end = ' ')\n",
    "    \n",
    "# test\n",
    "Z1 = W1 @ X_test + b1\n",
    "A1 = 1 / (1 + np.exp(-Z1))\n",
    "Z2 = W2 @ A1 + b2\n",
    "Z2 = Z2 - Z2.sum(axis = 0)\n",
    "A2 = np.exp(Z2)\n",
    "A2 = A2/A2.sum(axis = 0)\n",
    "\n",
    "# results\n",
    "preds = np.argmax(A2, axis = 0)\n",
    "truth = np.argmax(y_test, axis = 0)\n",
    "\n",
    "print('\\n\\nclassification_report')\n",
    "print(classification_report(truth, preds))\n",
    "print('\\n\\naccuracy_score')\n",
    "print(accuracy_score(truth, preds))\n",
    "print('\\n\\nconfusion_matrix')\n",
    "print(confusion_matrix(truth, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
