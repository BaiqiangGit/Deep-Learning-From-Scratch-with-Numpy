{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GAN from numpy\n",
    "\n",
    "Content derived from this reference blog:\n",
    "\n",
    "https://towardsdatascience.com/only-numpy-implementing-gan-general-adversarial-networks-and-adam-optimizer-using-numpy-with-2a7e4e032021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libs\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "random_numer = int(12345)\n",
    "np.random.seed(random_numer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## relu\n",
    "def ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask *x\n",
    "def d_ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask \n",
    "\n",
    "## arctan\n",
    "def arctan(x):\n",
    "    return np.arctan(x)\n",
    "def d_arctan(x):\n",
    "    return 1 / (1 + x ** 2)\n",
    "\n",
    "## tanh\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def d_tanh(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "## sigmoid\n",
    "def log(x):\n",
    "    return 1 / ( 1+ np.exp(-1*x))\n",
    "def d_log(x):\n",
    "    return log(x) * (1 - log(x))\n",
    "\n",
    "## make weight and bias tensor\n",
    "def makeWeight(d1, d2, factor = 0.002):\n",
    "    '''\n",
    "    d1: 1st Dimension\n",
    "    d2: 2nd Dimension\n",
    "    factor: the factor to control the value range [-factor, factor]\n",
    "    '''\n",
    "    return np.random.normal(size = (d1, d2), scale = 1/np.sqrt(d1/2)) * factor\n",
    "   \n",
    "def makeBias(d, factor = 0.002, flag = 'zeros'):\n",
    "    if flag == 'zeros':\n",
    "        return np.zeros(d)\n",
    "    elif flag == 'normal':\n",
    "        return np.random.normal(size = (d), scale = 1/np.sqrt(d/2)) * factor\n",
    "    else:\n",
    "        raise Exception(\"Bias should be initialized with only 'zeros' or 'normal' flags!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Data and declare hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Load Data --------------------------------\n",
      "-------- Data shape: (10000, 784) , Labels shape\" (10000,)\n",
      "-------- Data Loaded --------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------- Load Data --------------------------------')\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "images, labels = X_test.reshape(-1, 28*28), y_test ## use test set\n",
    "images, labels = shuffle(np.asarray(images),np.asarray(labels)) ## shuffle together\n",
    "print('-------- Data shape:', images.shape, ', Labels shape\"', labels.shape)\n",
    "print('-------- Data Loaded --------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define parameters\n",
    "G_input = 100\n",
    "hidden_input1 = 128\n",
    "hidden_input2 = 256 \n",
    "hidden_input3 = 346\n",
    "hidden_input4 = 480\n",
    "hidden_input5 = 560\n",
    "hidden_input6 = 686\n",
    "hidden_input7 = 784 ## back to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Descriminator Network ----------\n",
      "Arch: 784 x 128 x 1\n"
     ]
    }
   ],
   "source": [
    "## Build Descriminator\n",
    "print('--------- Descriminator Network ----------')\n",
    "print('Arch: 784 x 128 x 1')\n",
    "# 2. Declare Weights\n",
    "D_W1 = makeWeight(784, hidden_input1)\n",
    "D_W2 = makeWeight(hidden_input1, 1)\n",
    "D_b1 = makeBias(hidden_input1)\n",
    "D_b2 = makeBias(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Generator Network ----------\n",
      "Arch: 100 x 128 x 256 x 346 x 480 x 560 x 686 x 784\n"
     ]
    }
   ],
   "source": [
    "## Build Generator\n",
    "print('--------- Generator Network ----------')\n",
    "print('Arch: 100 x 128 x 256 x 346 x 480 x 560 x 686 x 784')\n",
    "G_W1 = makeWeight(G_input, hidden_input1)\n",
    "G_W2 = makeWeight(hidden_input1,hidden_input2)\n",
    "G_W3 = makeWeight(hidden_input2,hidden_input3)\n",
    "G_W4 = makeWeight(hidden_input3,hidden_input4)\n",
    "G_W5 = makeWeight(hidden_input4,hidden_input5)\n",
    "G_W6 = makeWeight(hidden_input5,hidden_input6)\n",
    "G_W7 = makeWeight(hidden_input6,hidden_input7)\n",
    "\n",
    "G_b1 = makeBias(hidden_input1)\n",
    "G_b2 = makeBias(hidden_input2)\n",
    "G_b3 = makeBias(hidden_input3)\n",
    "G_b4 = makeBias(hidden_input4)\n",
    "G_b5 = makeBias(hidden_input5)\n",
    "G_b6 = makeBias(hidden_input6)\n",
    "G_b7 = makeBias(hidden_input7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a tuple of dictions\n",
    "D_params = (dict({'W':D_W1,'dW':D_W1 * 0, 'b': D_b1, 'db': D_b1 * 0, 'a': ReLu, 'da': d_ReLu}), \n",
    "            dict({'W':D_W2,'dW':D_W2 * 0, 'b': D_b2, 'db': D_b2 * 0, 'a': log , 'da': d_log})) \n",
    "\n",
    "\n",
    "G_params = (dict({'W':G_W1, 'dW':G_W1 * 0, 'b': G_b1, 'db': G_b1 * 0, 'a': arctan, 'da': d_arctan}), \n",
    "            dict({'W':G_W2, 'dW':G_W2 * 0, 'b': G_b2, 'db': G_b2 * 0, 'a': ReLu  , 'da': d_ReLu}), \n",
    "            dict({'W':G_W3, 'dW':G_W3 * 0, 'b': G_b3, 'db': G_b3 * 0, 'a': arctan, 'da': d_arctan}), \n",
    "            dict({'W':G_W4, 'dW':G_W4 * 0, 'b': G_b4, 'db': G_b4 * 0, 'a': ReLu  , 'da': d_ReLu}), \n",
    "            dict({'W':G_W5, 'dW':G_W5 * 0, 'b': G_b5, 'db': G_b5 * 0, 'a': tanh  , 'da': d_tanh}), \n",
    "            dict({'W':G_W6, 'dW':G_W6 * 0, 'b': G_b6, 'db': G_b6 * 0, 'a': ReLu  , 'da': d_ReLu}), \n",
    "            dict({'W':G_W7, 'dW':G_W7 * 0, 'b': G_b7, 'db': G_b7 * 0, 'a': log   , 'da': d_log})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(Input, Params):\n",
    "    Output = Input.copy()\n",
    "    for layer in Params:\n",
    "        Output = Output.dot(layer['W']) + layer['b']\n",
    "        if layer['a']:\n",
    "            Output = layer['a'](Output)\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateVMP(V, M, Params, lr, eps):\n",
    "    # ---- Update Gradient G ----\n",
    "    assert len(V) == len(M)\n",
    "    \n",
    "    ## update V, M\n",
    "    for idx in range(len(V)):\n",
    "        if idx % 2 == 0:\n",
    "            M[idx] = beta_1 * M[idx] + (1 - beta_1) * Params[idx//2]['dW']\n",
    "            V[idx] = beta_2 * V[idx] + (1 - beta_2) * Params[idx//2]['dW'] ** 2            \n",
    "        else:\n",
    "            M[idx] = beta_1 * M[idx] + (1 - beta_1) * Params[idx//2]['db']\n",
    "            V[idx] = beta_2 * V[idx] + (1 - beta_2) * Params[idx//2]['db'] ** 2  \n",
    "\n",
    "    ## update Params\n",
    "    for idx, layer in enumerate(Params):\n",
    "        layer['W'] = layer['W'] - (lr/(np.sqrt(V[idx*2]  /(1-beta_2)) + eps)) * (M[idx*2]/(1-beta_1))\n",
    "        layer['b'] = layer['b'] - (lr/(np.sqrt(V[idx*2+1]/(1-beta_2)) + eps)) * (M[idx*2 + 1]/(1-beta_1))\n",
    "    \n",
    "    return V, M, Params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. For Adam Optimzier\n",
    "VD = [0] * 4\n",
    "MD = [0] * 4\n",
    "VG = [0] * 14\n",
    "MG = [0] * 14\n",
    "\n",
    "beta_1 = 0.99\n",
    "beta_2 = 0.999\n",
    "num_epoch = 10\n",
    "learing_rate = 1e-4\n",
    "eps    = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Started Training ----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dl2_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ec5e45ccf3dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Func Back D: Gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgrad_f_w2_part_1\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mDl2_fA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgrad_f_w2_part_2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0md_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDl2_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mgrad_f_w2_part_3\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mDl1_fA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mgrad_f_w2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_f_w2_part_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_f_w2_part_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_f_w2_part_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dl2_f' is not defined"
     ]
    }
   ],
   "source": [
    "print('--------- Started Training ----------')\n",
    "for iteration in range(num_epoch):\n",
    "\n",
    "    ## random sampling an image\n",
    "    random_int = np.random.randint(len(images) - 5)\n",
    "    current_image = np.expand_dims(images[random_int],axis=0) ## to tensor: (1, 784)\n",
    "\n",
    "    # Func G: Generate The first Fake Data\n",
    "    Z   = np.random.uniform(-1., 1., size=[1, G_input])\n",
    "    current_fake_data = forwardPass(Z, G_params)\n",
    "\n",
    "    # Func D: Forward Feed for Real and Fake data, calculate the cost\n",
    "    Dl2_rA = forwardPass(current_image, D_params)\n",
    "    Dl2_fA = forwardPass(current_fake_data, D_params)\n",
    "    D_cost = - np.log(Dl2_rA) + np.log(1.0- Dl2_fA) # Func: Cost D (GAN Loss)\n",
    "\n",
    "    # Func Back D: Gradient\n",
    "    grad_f_w2_part_1 =  1/(1.0- Dl2_fA)\n",
    "    grad_f_w2_part_2 =  d_log(Dl2_f)\n",
    "    grad_f_w2_part_3 =  Dl1_fA\n",
    "    grad_f_w2 = grad_f_w2_part_3.T.dot(grad_f_w2_part_1 * grad_f_w2_part_2) \n",
    "    grad_f_b2 = grad_f_w2_part_1 * grad_f_w2_part_2\n",
    "\n",
    "    grad_f_w1_part_1 =  (grad_f_w2_part_1 * grad_f_w2_part_2).dot(D_W2.T)\n",
    "    grad_f_w1_part_2 =  d_ReLu(Dl1_f)\n",
    "    grad_f_w1_part_3 =   current_fake_data\n",
    "    grad_f_w1 =       grad_f_w1_part_3.T.dot(grad_f_w1_part_1 * grad_f_w1_part_2) \n",
    "    grad_f_b1 =      grad_f_w1_part_1 * grad_f_w1_part_2\n",
    "\n",
    "    grad_r_w2_part_1 =  - 1/Dl2_rA\n",
    "    grad_r_w2_part_2 =  d_log(Dl2_r)\n",
    "    grad_r_w2_part_3 =   Dl1_rA\n",
    "    grad_r_w2 =       grad_r_w2_part_3.T.dot(grad_r_w2_part_1 * grad_r_w2_part_2) \n",
    "    grad_r_b2 =       grad_r_w2_part_1 * grad_r_w2_part_2\n",
    "\n",
    "    grad_r_w1_part_1 =  (grad_r_w2_part_1 * grad_r_w2_part_2).dot(D_W2.T)\n",
    "    grad_r_w1_part_2 =  d_ReLu(Dl1_r)\n",
    "    grad_r_w1_part_3 =   current_image\n",
    "    grad_r_w1 =       grad_r_w1_part_3.T.dot(grad_r_w1_part_1 * grad_r_w1_part_2) \n",
    "    grad_r_b1 =       grad_r_w1_part_1 * grad_r_w1_part_2\n",
    "\n",
    "    grad_w1 = D_params[0]['dW'] + grad_r_w1\n",
    "    grad_b1 = D_params[0]['db'] + grad_r_b1    \n",
    "    grad_w2 = D_params[1]['dW'] + grad_r_w2\n",
    "    grad_b2 = D_params[1]['db'] + grad_r_b2\n",
    "    \n",
    "    ## to container\n",
    "    D_params[0]['dW'] = grad_w1\n",
    "    D_params[0]['db'] = grad_b1\n",
    "    D_params[1]['dW'] = grad_w2\n",
    "    D_params[1]['db'] = grad_w2    \n",
    "\n",
    "    # ---- Update Gradient D ----\n",
    "    VD, MD, D_params = updateVMP(VD, MD, D_params, learing_rate, eps)\n",
    "    D_W1 = D_params[0]['W']\n",
    "    D_b1 = D_params[0]['b']\n",
    "    D_W2 = D_params[1]['W']\n",
    "    D_b2 = D_params[1]['b']\n",
    "\n",
    "    # Func G: Forward Feed for G\n",
    "    Z = np.random.uniform(-1., 1., size=[1, G_input])\n",
    "    Gl1 = Z.dot(G_W1) + G_b1\n",
    "    Gl1A = arctan(Gl1)\n",
    "    Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "    Gl2A = ReLu(Gl2)\n",
    "    Gl3 = Gl2A.dot(G_W3) + G_b3\n",
    "    Gl3A = arctan(Gl3)\n",
    "\n",
    "    Gl4 = Gl3A.dot(G_W4) + G_b4\n",
    "    Gl4A = ReLu(Gl4)\n",
    "    Gl5 = Gl4A.dot(G_W5) + G_b5\n",
    "    Gl5A = tanh(Gl5)\n",
    "    Gl6 = Gl5A.dot(G_W6) + G_b6\n",
    "    Gl6A = ReLu(Gl6)\n",
    "    Gl7 = Gl6A.dot(G_W7) + G_b7\n",
    "    \n",
    "    current_fake_data = log(Gl7)\n",
    "\n",
    "    Dl1 = current_fake_data.dot(D_W1) + D_b1\n",
    "    Dl1_A = ReLu(Dl1)\n",
    "    Dl2 = Dl1_A.dot(D_W2) + D_b2\n",
    "    Dl2_A = log(Dl2)\n",
    "\n",
    "    # Func: Cost G\n",
    "    G_cost = -np.log(Dl2_A)\n",
    "\n",
    "    # Func: Gradient\n",
    "    grad_G_w7_part_1 = ((-1/Dl2_A) * d_log(Dl2).dot(D_W2.T) * (d_ReLu(Dl1))).dot(D_W1.T)\n",
    "    grad_G_w7_part_2 = d_log(Gl7)\n",
    "    grad_G_w7_part_3 = Gl6A\n",
    "    grad_G_w7 = grad_G_w7_part_3.T.dot(grad_G_w7_part_1 * grad_G_w7_part_1)\n",
    "    grad_G_b7 = grad_G_w7_part_1 * grad_G_w7_part_2\n",
    "\n",
    "    grad_G_w6_part_1 = (grad_G_w7_part_1 * grad_G_w7_part_2).dot(G_W7.T)\n",
    "    grad_G_w6_part_2 = d_ReLu(Gl6)\n",
    "    grad_G_w6_part_3 = Gl5A\n",
    "    grad_G_w6 = grad_G_w6_part_3.T.dot(grad_G_w6_part_1 * grad_G_w6_part_2)\n",
    "    grad_G_b6 = (grad_G_w6_part_1 * grad_G_w6_part_2)\n",
    "\n",
    "    grad_G_w5_part_1 = (grad_G_w6_part_1 * grad_G_w6_part_2).dot(G_W6.T)\n",
    "    grad_G_w5_part_2 = d_tanh(Gl5)\n",
    "    grad_G_w5_part_3 = Gl4A\n",
    "    grad_G_w5 = grad_G_w5_part_3.T.dot(grad_G_w5_part_1 * grad_G_w5_part_2)\n",
    "    grad_G_b5 = (grad_G_w5_part_1 * grad_G_w5_part_2)\n",
    "\n",
    "    grad_G_w4_part_1 = (grad_G_w5_part_1 * grad_G_w5_part_2).dot(G_W5.T)\n",
    "    grad_G_w4_part_2 = d_ReLu(Gl4)\n",
    "    grad_G_w4_part_3 = Gl3A\n",
    "    grad_G_w4 = grad_G_w4_part_3.T.dot(grad_G_w4_part_1 * grad_G_w4_part_2)\n",
    "    grad_G_b4 = (grad_G_w4_part_1 * grad_G_w4_part_2)\n",
    "\n",
    "    grad_G_w3_part_1 = (grad_G_w4_part_1 * grad_G_w4_part_2).dot(G_W4.T)\n",
    "    grad_G_w3_part_2 = d_arctan(Gl3)\n",
    "    grad_G_w3_part_3 = Gl2A\n",
    "    grad_G_w3 = grad_G_w3_part_3.T.dot(grad_G_w3_part_1 * grad_G_w3_part_2)\n",
    "    grad_G_b3 = (grad_G_w3_part_1 * grad_G_w3_part_2)\n",
    "\n",
    "    grad_G_w2_part_1 = (grad_G_w3_part_1 * grad_G_w3_part_2).dot(G_W3.T)\n",
    "    grad_G_w2_part_2 = d_ReLu(Gl2)\n",
    "    grad_G_w2_part_3 = Gl1A\n",
    "    grad_G_w2 = grad_G_w2_part_3.T.dot(grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "    grad_G_b2 = (grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "\n",
    "    grad_G_w1_part_1 = (grad_G_w2_part_1 * grad_G_w2_part_2).dot(G_W2.T)\n",
    "    grad_G_w1_part_2 = d_arctan(Gl1)\n",
    "    grad_G_w1_part_3 = Z\n",
    "    grad_G_w1 = grad_G_w1_part_3.T.dot(grad_G_w1_part_1 * grad_G_w1_part_2)\n",
    "    grad_G_b1 = grad_G_w1_part_1 * grad_G_w1_part_2\n",
    "\n",
    "    # ---- Update Gradient G ---- \n",
    "    VG, MG, G_params = updateVMP(VG, MG, G_params, learing_rate, eps)\n",
    "    G_W1 = G_params[0]['W']\n",
    "    G_b1 = G_params[0]['b']\n",
    "    \n",
    "    G_W2 = G_params[1]['W']\n",
    "    G_b2 = G_params[1]['b']\n",
    "    \n",
    "    G_W3 = G_params[2]['W']\n",
    "    G_b3 = G_params[2]['b']\n",
    "    \n",
    "    G_W4 = G_params[3]['W']\n",
    "    G_b4 = G_params[3]['b']\n",
    "    \n",
    "    G_W5 = G_params[4]['W']\n",
    "    G_b5 = G_params[4]['b']\n",
    "    \n",
    "    G_W6 = G_params[5]['W']\n",
    "    G_b6 = G_params[5]['b']\n",
    "    \n",
    "    G_W7 = G_params[6]['W']\n",
    "    G_b7 = G_params[6]['b']\n",
    "    \n",
    "    # --- Print Error ----\n",
    "    print(\"Current Iter: \",iteration, \" Current D cost:\",D_cost, \" Current G cost: \", G_cost,end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
