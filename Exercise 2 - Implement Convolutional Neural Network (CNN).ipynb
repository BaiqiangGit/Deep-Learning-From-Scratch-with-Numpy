{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Intro\n",
    "\n",
    "With no doubt, Convolutional Neural Networks (CNN) has been the most successful model for computer vision tasks. \n",
    "\n",
    "Convolutional operation has similar mechanism to the way that human eyes work in visual perception. When humans explore the visual world, the eyes behave in a pattern of alternative fixations and saccades. The saccadic eye movements bring the visual target to the fovea abruptly (about 20ms), and the target information is then processed during eye fixations when the eyes stay relatively stable (e.g 200ms). We are usually un-aware of the eye movements, as they are programed and executed automatically by cognitive brain process. Our brain then aggregate all these local information to a global decision, based on previous knowledge/experience. The visual field is not explored as a whole. Only a selective set of local positions are viewed, and that turns out to be enough to serve the perception needs in our daily lives (It means images are extremely redundant to serve the recognition/classification popurse. Duplicated and irrelavant information should be effectively discarded to gain efficiency, e.g through weighting and local operator (local operators also can be considered as weighting by penalizing weights of the positions outside the receptive field to 0). Images are too rich and also too costy.).\n",
    "\n",
    "From this perspective, CNN is very much bio-inspired methodologyï¼š local-to-global, like divide-and-conquer (e.g to sort a list, you can sort the sublists (local) then merge to have the global solution). It acts like information selector and aggragator, grab the needed and throw away the rest. \n",
    "\n",
    "OK, too much talking, stop brain storming and code it.\n",
    "\n",
    "#### Let code say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load libs\n",
    "%matplotlib inline\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = fetch_mldata('mnist original', data_home = 'datasets/')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "X.shape, y.shape ## shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 70000), (10, 70000))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.T\n",
    "X = X / 255.0\n",
    "Y = OneHotEncoder().fit_transform(y.reshape(-1,1).astype('int32')).toarray().T\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 28, 28, 60000), (10, 60000), (1, 28, 28, 10000), (10, 10000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 60000\n",
    "X_train, X_test = X[:,:m].reshape(1,28,28,-1), X[:,m:].reshape(1,28,28,-1)\n",
    "Y_train, Y_test = Y[:,:m], Y[:,m:]\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 28, 28, 60000), (10, 60000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(54321)\n",
    "shuffle = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:,:,:,shuffle], Y_train[:,shuffle]\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 134\n",
    "plt.imshow(X_train[:,:,:,idx].squeeze(), cmap = 'binary_r')\n",
    "plt.title(np.argmax(Y_train[:,idx]))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input layer\n",
    "input_depth  = 1\n",
    "input_height = 28\n",
    "input_width  = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convolution layer\n",
    "conv_depth  = 2\n",
    "conv_height = 3\n",
    "conv_width  = 3\n",
    "\n",
    "## trainable parameters connecting input & convolution layers\n",
    "W1 = np.random.randn(conv_depth, input_depth, conv_height, conv_width)\n",
    "b1 = np.zeros((conv_depth, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## densely connected (fc) layer\n",
    "fc_dims = 32\n",
    "flatten_dims = conv_depth * (input_height - conv_height + 1) * (input_width - conv_width + 1)\n",
    "\n",
    "## trainable parameters connecting convolution & dense layers\n",
    "W2 = np.random.randn(fc_dims, flatten_dims)\n",
    "b2 = np.zeros((fc_dims, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output layer\n",
    "output_dims = 10\n",
    "\n",
    "## trainable parameters connecting dense & output layers\n",
    "W3 = np.random.randn(output_dims, fc_dims)\n",
    "b3 = np.zeros((output_dims, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 28, 28, 60000), (10, 60000))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prepare inputs\n",
    "Input  = X_train.copy()\n",
    "Target = Y_train.copy()\n",
    "Input.shape, Target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , loss: [1.28140686 0.20822068 0.2338827  0.46949416 0.60362163 0.17409459\n",
      " 0.35062963 1.02211311 1.32937071 0.04804767]\n",
      "epoch: 1 , loss: [0.38369461 0.06354422 0.77907208 0.20330568 0.38941393 0.18225178\n",
      " 0.46335044 0.22145518 0.68721877 1.18083523]\n",
      "epoch: 2 , loss: [0.34118477 0.98553669 0.58973446 0.09616901 0.11638067 0.16605581\n",
      " 0.25018618 0.26129972 0.40057913 0.76627435]\n",
      "epoch: 3 , loss: [0.13881807 0.66915701 0.38335981 0.55071828 0.3880977  0.23016209\n",
      " 0.24817241 0.08562567 0.22102089 0.52127314]\n",
      "epoch: 4 , loss: [0.55372952 0.38240065 0.14696803 0.54680683 0.17156462 0.17519124\n",
      " 0.13035284 0.70305301 0.20451274 0.36796369]\n",
      "epoch: 5 , loss: [0.34151146 0.16762261 0.26628403 0.32332559 0.20978761 0.18224469\n",
      " 0.3076453  0.46574045 0.17083644 0.18504516]\n",
      "epoch: 6 , loss: [0.21586048 0.32785749 0.19406651 0.20420862 0.21118384 0.21637319\n",
      " 0.20095451 0.29251818 0.2560406  0.24154615]\n",
      "epoch: 7 , loss: [0.23242883 0.22555531 0.25360815 0.25162868 0.23244112 0.21426809\n",
      " 0.24437044 0.2203629  0.21580981 0.22066201]\n",
      "epoch: 8 , loss: [0.22568329 0.2612961  0.22039339 0.22442451 0.22329748 0.2172761\n",
      " 0.22149135 0.24540321 0.23199796 0.23268553]\n",
      "epoch: 9 , loss: [0.23015672 0.23679541 0.23447607 0.23837881 0.22862081 0.21729422\n",
      " 0.23231189 0.23134363 0.22498903 0.22764145]\n",
      "epoch: 10 , loss: [0.22758303 0.25200254 0.22677901 0.23018803 0.22573063 0.21695097\n",
      " 0.22648256 0.23839846 0.22770123 0.22964755]\n",
      "epoch: 11 , loss: [0.22917324 0.24169142 0.23076751 0.23478301 0.22740242 0.21740039\n",
      " 0.22956132 0.23468208 0.22678078 0.22901753]\n",
      "epoch: 12 , loss: [0.22819881 0.24834426 0.22857501 0.23208175 0.22642583 0.21705683\n",
      " 0.22785971 0.23653148 0.2270004  0.2291191 ]\n",
      "epoch: 13 , loss: [0.2288109  0.24388698 0.22978328 0.23365703 0.22701621 0.21730915\n",
      " 0.22881446 0.2356316  0.22704205 0.2292138 ]\n",
      "epoch: 14 , loss: [0.22842485 0.24680646 0.22909761 0.23271827 0.22665451 0.21713782\n",
      " 0.22826391 0.23604126 0.22693296 0.22907742]\n",
      "epoch: 15 , loss: [0.22867101 0.24486233 0.22949335 0.23328011 0.22688052 0.21725326\n",
      " 0.2285881  0.23587407 0.22704354 0.22920425]\n",
      "epoch: 16 , loss: [0.22851321 0.24614424 0.22926003 0.23293913 0.2267379  0.21717704\n",
      " 0.22839324 0.23592658 0.22695367 0.22910357]\n",
      "epoch: 17 , loss: [0.22861485 0.24529276 0.22940014 0.23314757 0.22682893 0.21722734\n",
      " 0.22851247 0.23592458 0.2270208  0.22917828]\n",
      "epoch: 18 , loss: [0.22854897 0.24585603 0.22931462 0.23301865 0.22677045 0.21719432\n",
      " 0.22843837 0.23590762 0.22697309 0.22912515]\n",
      "epoch: 19 , loss: [0.22859166 0.24548223 0.22936765 0.23309886 0.22680828 0.21721603\n",
      " 0.22848499 0.23592912 0.22700614 0.22916205]\n"
     ]
    }
   ],
   "source": [
    "## initialize convolution output\n",
    "conv_output_height = input_height - conv_height + 1\n",
    "conv_output_width  = input_width  - conv_width + 1\n",
    "conv_output = np.zeros((conv_depth, conv_output_height, conv_output_width, Input.shape[-1]))\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    #------------------------------------------------------------------FORWARD BLOCK\n",
    "    ## feed forward: convolution operation\n",
    "    for f in range(conv_depth):\n",
    "        for r in range(conv_output_height):\n",
    "            for c in range(conv_output_width):\n",
    "                current_patch  = Input[:, r : r + conv_height, c : c + conv_width]\n",
    "                current_filter = np.expand_dims(W1[f,:,:,:], axis = 3) ## to match shape for broadcasting\n",
    "                conv_output[f, r, c] = (current_patch * current_filter + b1[f]).reshape(-1, Input.shape[-1]).sum(axis = 0) ## reshape 2X faster\n",
    "                # conv_output[f, r, c] += (current_patch * current_filter + b1[f]).sum(axis = 0).sum(axis = 0).sum(axis = 0)\n",
    "\n",
    "    ## feed forward: flatten the convolution output\n",
    "    conv_output_flatten = conv_output.reshape(-1, Input.shape[-1])\n",
    "    A1 = 1 / (1 + np.exp(-conv_output_flatten)) ## sigmoid\n",
    "\n",
    "    ## feed forward: affine operation\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = 1/(1 + np.exp(-Z2))\n",
    "\n",
    "    ## geed forward: affine + softmax operation\n",
    "    Z3 = W3 @ A2 + b3\n",
    "    Z3 = Z3 - np.max(Z3, axis = 0)\n",
    "    A3 = np.exp(Z3)/np.exp(Z3).sum(axis = 0)\n",
    "\n",
    "    \n",
    "    #------------------------------------------------------------------BACKWARD  BLOCK\n",
    "    ## backpropagation: softmax layer\n",
    "    dZ3 = A3 - Y_train\n",
    "    dW3 = dZ3 @ A2.T / Input.shape[-1]\n",
    "    db3 = dZ3.mean(axis = 1, keepdims = True)\n",
    "\n",
    "    ## backpropagation: dense layer\n",
    "    dA2 = W3.T @ dZ3\n",
    "    dZ2 = dA2 * A2 * (1 - A2)\n",
    "    dW2 = dZ2 @ A1.T / Input.shape[-1]\n",
    "    db2 = dZ2.mean(axis = 1, keepdims = True)\n",
    "\n",
    "    ## backpropagation: convolution layer\n",
    "    dA1 = W2.T @ dZ2\n",
    "    d_conv_flatten = dA1 * A1 * (1 - A1)\n",
    "    d_conv_matrix = d_conv_flatten.reshape(conv_output.shape)\n",
    "\n",
    "    ## backpropagation: convolution layer --> weight\n",
    "    dW1 = np.zeros(W1.shape)\n",
    "    for in_c in range(Input.shape[0]):\n",
    "        for out_c in range(conv_output.shape[0]):\n",
    "            for r in range(conv_height):\n",
    "                for c in range(conv_width):\n",
    "                    conv_input_patch = Input[in_c, r : r + conv_output_height, c : c + conv_output_width, :] ## conv input\n",
    "                    conv_output_vals = d_conv_matrix[out_c] ## conv results\n",
    "                    dW1[out_c, in_c, r, c] = np.sum(conv_input_patch * conv_output_vals)/Input.shape[-1]\n",
    "\n",
    "    ## backpropagation: convolution layer --> bias\n",
    "    db1 = d_conv_matrix.sum(axis = 1).sum(axis = 1).mean(axis = 1, keepdims = True)\n",
    "\n",
    "    # equivalent\n",
    "    # db1 = np.zeros((b1.shape))\n",
    "    # for out_c in range(d_conv_matrix.shape[0]):\n",
    "    #    db1[out_c] += d_conv_matrix[out_c].sum()/Input.shape[-1]\n",
    "\n",
    "    ## backpropagation: convolution layer --> Input\n",
    "    dInput = np.zeros_like(Input)\n",
    "    for in_c in range(Input.shape[0]):\n",
    "        for out_c in range(conv_output.shape[0]):\n",
    "            current_filter = np.expand_dims(W1[out_c, in_c], axis = 2)\n",
    "            for r in range(conv_output_height):\n",
    "                for c in range(conv_output_width):\n",
    "                    d_conv_val = d_conv_matrix[out_c, in_c, r, c]\n",
    "                    dInput[in_c, r : r+conv_height, c : c + conv_width, :] += d_conv_val * current_filter\n",
    "\n",
    "    #------------------------------------------------------------------ UPDATE PARAMETERS\n",
    "    ## update model\n",
    "    lr = 1\n",
    "    W3 -= dW3 * lr\n",
    "    W2 -= dW2 * lr\n",
    "    W1 -= dW1 * lr\n",
    "    b3 -= db3 * lr\n",
    "    b2 -= db2 * lr\n",
    "    b1 -= db1 * lr\n",
    "\n",
    "    ## compute loss\n",
    "    Loss = -np.mean(Y_train * np.log(A3), axis = 1)\n",
    "\n",
    "    print('epoch:', epoch, ', loss:', Loss.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize convolution output\n",
    "Input = X_test.copy()\n",
    "conv_output_height = input_height - conv_height + 1\n",
    "conv_output_width  = input_width  - conv_width + 1\n",
    "conv_output = np.zeros((conv_depth, conv_output_height, conv_output_width, Input.shape[-1]))\n",
    "\n",
    "## feed forward: convolution operation\n",
    "for f in range(conv_depth):\n",
    "    for r in range(conv_output_height):\n",
    "        for c in range(conv_output_width):\n",
    "            current_patch  = Input[:, r : r + conv_height, c : c + conv_width]\n",
    "            current_filter = np.expand_dims(W1[f,:,:,:], axis = 3) ## to match shape for broadcasting\n",
    "            conv_output[f, r, c] = (current_patch * current_filter + b1[f]).reshape(-1, Input.shape[-1]).sum(axis = 0) ## reshape 2X faster\n",
    "            # conv_output[f, r, c] += (current_patch * current_filter + b1[f]).sum(axis = 0).sum(axis = 0).sum(axis = 0)\n",
    "\n",
    "## feed forward: flatten the convolution output\n",
    "conv_output_flatten = conv_output.reshape(-1, Input.shape[-1])\n",
    "A1 = 1 / (1 + np.exp(-conv_output_flatten)) ## sigmoid\n",
    "\n",
    "## feed forward: affine operation\n",
    "Z2 = W2 @ A1 + b2\n",
    "A2 = 1/(1 + np.exp(-Z2))\n",
    "\n",
    "## geed forward: affine + softmax operation\n",
    "Z3 = W3 @ A2 + b3\n",
    "Z3 = Z3 - np.max(Z3, axis = 0)\n",
    "A3 = np.exp(Z3)/np.exp(Z3).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(A3, axis = 0)\n",
    "truth = np.argmax(Y_test, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1135\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(truth, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  980    0    0    0    0    0    0    0    0]\n",
      " [   0 1135    0    0    0    0    0    0    0    0]\n",
      " [   0 1032    0    0    0    0    0    0    0    0]\n",
      " [   0 1010    0    0    0    0    0    0    0    0]\n",
      " [   0  982    0    0    0    0    0    0    0    0]\n",
      " [   0  892    0    0    0    0    0    0    0    0]\n",
      " [   0  958    0    0    0    0    0    0    0    0]\n",
      " [   0 1028    0    0    0    0    0    0    0    0]\n",
      " [   0  974    0    0    0    0    0    0    0    0]\n",
      " [   0 1009    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(truth, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       980\n",
      "           1       0.11      1.00      0.20      1135\n",
      "           2       0.00      0.00      0.00      1032\n",
      "           3       0.00      0.00      0.00      1010\n",
      "           4       0.00      0.00      0.00       982\n",
      "           5       0.00      0.00      0.00       892\n",
      "           6       0.00      0.00      0.00       958\n",
      "           7       0.00      0.00      0.00      1028\n",
      "           8       0.00      0.00      0.00       974\n",
      "           9       0.00      0.00      0.00      1009\n",
      "\n",
      "   micro avg       0.11      0.11      0.11     10000\n",
      "   macro avg       0.01      0.10      0.02     10000\n",
      "weighted avg       0.01      0.11      0.02     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(truth, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
